{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1b830d-2ead-40ff-8d12-c43f3d0e2d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install llmcompressor Tool\n",
    "!pip install llmcompressor==0.5.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd550fee-758b-4281-a628-5cb60378bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmcompressor                            0.5.2\n"
     ]
    }
   ],
   "source": [
    "#Validate LLM Compressor is installed\n",
    "!pip list | grep llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730b0d19-51b3-4960-ad89-e353e35b3eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/app-root/lib64/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/opt/app-root/lib64/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-09T23:04:53.465794+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-11-09T23:04:53.468437+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2025-11-09T23:04:53.500973+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-11-09T23:04:53.501839+0000 | IndependentPipeline | INFO - Inferred `DataFreePipeline` for `QuantizationModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:00<00:00, 500175.45it/s]\n",
      "Calibrating weights: 100%|██████████| 293/293 [00:00<00:00, 285731.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-09T23:04:53.511108+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
      "2025-11-09T23:04:53.511625+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "2025-11-09T23:04:53.514632+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantised model saved to: TinyLlama/TinyLlama-1.1B-Chat-v1.0-W4A16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TinyLlama/TinyLlama-1.1B-Chat-v1.0-W4A16'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Python script to quantise the model\n",
    "\n",
    "from quantiser import quantise_model\n",
    "\n",
    "# You can view this code by opening the Python file from the File Explorer. \n",
    "quantise_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", scheme=\"W4A16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d4fdcd-d832-4bd3-9f16-3a14fe254ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.1G\n",
      "4.0K drwxr-sr-x. 2 1000870000 1000870000 4.0K Nov  9 23:05 .\n",
      "4.0K drwxr-sr-x. 3 1000870000 1000870000 4.0K Nov  9 23:04 ..\n",
      "4.0K -rw-r--r--. 1 1000870000 1000870000  675 Nov  9 23:04 config.json\n",
      "4.0K -rw-r--r--. 1 1000870000 1000870000  124 Nov  9 23:04 generation_config.json\n",
      "2.1G -rw-r--r--. 1 1000870000 1000870000 2.1G Nov  9 23:05 model.safetensors\n",
      "4.0K -rw-r--r--. 1 1000870000 1000870000  130 Nov  9 23:05 recipe.yaml\n"
     ]
    }
   ],
   "source": [
    "# Review contents of the file \n",
    "!ls -lsah TinyLlama/TinyLlama-1.1B-Chat-v1.0-W4A16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f55cc6f-41ac-4353-b71e-3d5e15d3efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy of the model. We will be using lm-eval script in vLLM.\n",
    "!pip install vllm==0.9.1 lm-eval==0.4.7 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee7517-c4ce-4cf4-a558-a5b599dbdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate unquantized model\n",
    "# !lm-eval --model vllm --model_args pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0,tensor_parallel_size=1 \\\n",
    "# --limit 250 --tasks hellaswag --num_fewshot 5 --batch_size 5\n",
    "!time lm-eval \\\n",
    "  --model vllm \\\n",
    "  --model_args pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0,tensor_parallel_size=1,gpu_memory_utilization=0.8 \\\n",
    "  --limit 250 \\\n",
    "  --tasks hellaswag \\\n",
    "  --num_fewshot 5 \\\n",
    "  --batch_size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea2199-a2b2-42bd-9e2c-961ce238e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the same command against the post-quantized model.\n",
    "# Note: You will need specify the Tokenizer explicitly.\n",
    "\n",
    "!time lm-eval \\\n",
    "  --model vllm \\\n",
    "  --model_args pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0-W4A16,tokenizer=TinyLlama/TinyLlama-1.1B-Chat-v1.0,tensor_parallel_size=1,gpu_memory_utilization=0.8 \\\n",
    "  --limit 250 \\\n",
    "  --tasks hellaswag \\\n",
    "  --num_fewshot 5 \\\n",
    "  --batch_size 5\n",
    "\n",
    "# !time lm-eval --model vllm --model_args pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0-W4A16,tokenizer=TinyLlama/TinyLlama-1.1B-Chat-v1.0,tensor_parallel_size=1 \\\n",
    "# --limit 250 --tasks hellaswag --num_fewshot 5 --batch_size 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e9b3c-8a85-41c1-91f3-592eb78248f0",
   "metadata": {},
   "source": [
    "By comparsing the pre-quantized-eval and post-quantized-eval,\n",
    "\n",
    "The value reduces from 0.572 to 0.564\n",
    "\n",
    "Accuracy Recovery = Post-quantized-accuracy / Pre-quantized-accuracy.\n",
    "\n",
    "So, the recovery here is 0.564 / 0.572 * 100% = 98.6%\n",
    "\n",
    "Acutally, you have to run more than 500,000 evaluations on quantized models, which retains about 99% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
