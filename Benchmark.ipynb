{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fc9085d6-fe44-4c37-a7fb-e97a05001e7b",
   "metadata": {},
   "source": [
    "# Step 1: Authenticate with Hugging Face for Gated Model\n",
    "# Open Terminal and authenticate to HuggingFace with Token\n",
    "# hf auth login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85282e9a-3ab7-4491-9d93-d79c0a89530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install vllm from pip\n",
    "# !pip install uv\n",
    "# !uv pip install -r requirements.txt\n",
    "\n",
    "# requirements.txt\n",
    "# vllm==0.9.1\n",
    "# llmcompressor==0.5.2\n",
    "# pandas==2.3.0\n",
    "# datasets==3.6.0\n",
    "# lm-eval==0.4.7\n",
    "# transformers<4.54.0 # https://github.com/vllm-project/vllm-ascend/issues/2046\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f3be2-baa5-4d2a-a5ca-8e5d8ae10b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96a073-dbd3-4e71-a250-5e2318f942a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a807382d-2a41-4c31-8938-a37ab8d02edf",
   "metadata": {},
   "source": [
    "# Step 2: Server the model by running vllm serve from terminal\n",
    "# To see options, please run vllm serve -h\n",
    "# Go to File, Terminal and from terminal run the command such as\n",
    "# vllm serve RedHatAI/Llama-3.2-1B-Instruct-FP8 --port 8000 --tensor-parallel-size 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514b152-0dcf-41ca-8afb-89ddc760b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to OC Cluster\n",
    "# !oc login --token=<Enter Token> --server=<Enter Server Detail>\n",
    "!oc login --token=sha256~xxxxxx --server=https://api.cluster-xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdf08f-a109-499f-bd92-9fe1456eabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the environment\n",
    "import subprocess\n",
    "import sys\n",
    "import pkg_resources\n",
    "from packaging import version\n",
    "import os\n",
    "\n",
    "def check_package_version(package_name, expected_version):\n",
    "    \"\"\"Check if a package is installed and has the expected version\"\"\"\n",
    "    try:\n",
    "        installed_version = pkg_resources.get_distribution(package_name).version\n",
    "        if version.parse(installed_version) >= version.parse(expected_version):\n",
    "            return True, installed_version\n",
    "        else:\n",
    "            return False, installed_version\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False, \"Not installed\"\n",
    "\n",
    "def check_llama_serving_status():\n",
    "    \"\"\"Check the status of llama-serving namespace\"\"\"\n",
    "    try:\n",
    "        # Check if we can access the namespace\n",
    "        result = subprocess.run( \n",
    "            [\"oc\", \"get\", \"pods\", \"-n\", \"llama-serving\", \"--no-headers\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            if not lines or lines == ['']:\n",
    "                return True, \"No pods found - default vLLM inference serving is disabled (optimal for labs)\"\n",
    "            \n",
    "            # Check pod status\n",
    "            pod_status = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        pod_name = parts[0]\n",
    "                        ready = parts[1]\n",
    "                        status = parts[2]\n",
    "                        pod_status.append(f\"{pod_name}: {ready} {status}\")\n",
    "            \n",
    "            return True, pod_status\n",
    "        else:\n",
    "            return False, f\"Error accessing namespace: {result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, \"Timeout accessing llama-serving namespace\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Main validation function\"\"\"\n",
    "    print(\"üîç Environment Validation Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check required packages\n",
    "    required_packages = {\n",
    "        \"vllm\": \"0.9.1\",\n",
    "        \"llmcompressor\": \"0.5.2\"\n",
    "    }\n",
    "    \n",
    "    all_checks_passed = True\n",
    "    \n",
    "    for package, expected_ver in required_packages.items():\n",
    "        is_valid, installed_ver = check_package_version(package, expected_ver)\n",
    "        \n",
    "        if is_valid:\n",
    "            print(f\"‚úÖ {package}: {installed_ver} (>= {expected_ver})\")\n",
    "        else:\n",
    "            print(f\"‚ùå {package}: {installed_ver} (expected >= {expected_ver})\")\n",
    "            all_checks_passed = False\n",
    "    \n",
    "    # Check llama-serving status\n",
    "    print(\"\\nüîç llama-serving Status:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    is_accessible, status_info = check_llama_serving_status()\n",
    "    \n",
    "    if is_accessible:\n",
    "        print(\"‚úÖ llama-serving namespace is accessible\")\n",
    "        if isinstance(status_info, list):\n",
    "            for pod_info in status_info:\n",
    "                print(f\"  üì¶ {pod_info}\")\n",
    "        else:\n",
    "            print(f\"  ‚ÑπÔ∏è  {status_info}\")\n",
    "    else:\n",
    "        print(f\"‚ùå llama-serving namespace: {status_info}\")\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if all_checks_passed:\n",
    "        print(\"üéâ All validation checks passed! Environment is ready.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some validation checks failed. Please review the issues above.\")\n",
    "    \n",
    "    return all_checks_passed\n",
    "\n",
    "# Run the validation\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cbb9f-6723-4df4-a3dd-e57166510452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status Check\n",
    "def quick_status_check():\n",
    "    \"\"\"Quick validation check with minimal output\"\"\"\n",
    "    print(\"üöÄ Quick Environment Status Check\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Check vllm\n",
    "    vllm_ok, vllm_ver = check_package_version(\"vllm\", \"0.9.1\")\n",
    "    print(f\"{'‚úÖ' if vllm_ok else '‚ùå'} vllm: {vllm_ver}\")\n",
    "    \n",
    "    # Check llmcompressor\n",
    "    llmc_ok, llmc_ver = check_package_version(\"llmcompressor\", \"0.5.2\")\n",
    "    print(f\"{'‚úÖ' if llmc_ok else '‚ùå'} llmcompressor: {llmc_ver}\")\n",
    "    \n",
    "    # Check llama-serving\n",
    "    llama_ok, _ = check_llama_serving_status()\n",
    "    print(f\"{'‚úÖ' if llama_ok else '‚ùå'} llama-serving: {'accessible, default vLLM inference serving is disabled (optimal for labs)' if llama_ok else 'issue detected'}\")\n",
    "    \n",
    "    # Overall status\n",
    "    all_good = vllm_ok and llmc_ok and llama_ok\n",
    "    print(f\"\\n{'üéâ' if all_good else '‚ö†Ô∏è'} Overall: {'All systems ready!' if all_good else 'Issues found - run full validation above'}\")\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "# Run quick check\n",
    "quick_status_check()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4e06fe9-8aac-4b80-a48c-0075bfe3761b",
   "metadata": {},
   "source": [
    "# Serve the model in Terminal\n",
    "#  vllm serve RedHatAI/Llama-3.2-1B-Instruct-FP8 --port 8000 --tensor-parallel-size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310687d-2b81-4bd3-a29d-f1597a23485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model is running\n",
    "!curl -X POST -H \"Content-Type: application/json\" -d '{ \\\n",
    "    \"prompt\": \"What is the capital of France?\", \\\n",
    "    \"max_tokens\": 50 \\\n",
    "}' http://localhost:8000/v1/completions | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca3516-5c9e-4af8-9f9c-f9aa6fe8dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Benchmarking Tool\n",
    "!git clone https://github.com/vllm-project/vllm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aaf24c-f4b2-4ab2-bf55-38d90652326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd vllm && git checkout v0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a632f3f-407e-4c17-ad04-b28206ec54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter token for Hugging Face\n",
    "# %env HF_TOKEN=<token>\n",
    "%env HF_TOKEN=hf_xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67aac2-a57b-4de9-8487-b05415f12eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Benchmarking tool\n",
    "!python vllm/benchmarks/benchmark_serving.py \\\n",
    "--backend vllm --model RedHatAI/Llama-3.2-1B-Instruct-FP8 \\\n",
    "--num-prompts 100 --dataset-name random  --random-input 200 --random-output 200 --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096d97e-863d-440a-be7c-a10c076c1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Nvidia Benchmarking Performance\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://docs.nvidia.com/nim/benchmarking/llm/latest/performance.html#llama-3-1-8b-instruct-results\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c289f-fada-421e-a6a5-18e0d5ac589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect what are you running on OpenShift Cluster\n",
    "!oc project ai-roadshow\n",
    "!oc get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f6b91-c3da-4383-8ed1-690b820de614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the nvidia-smi utility in the Pod to retrieve the details of the NVIDIA GPU\n",
    "!oc exec vllm-0 -- nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2800a1c-f8d6-4c63-8fdb-d976d4c4a401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
